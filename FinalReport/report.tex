% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{FinalReport/report}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{natbib}

\title{15-618 Final Project: Sparse Attention in CUDA}

\author{Sarah Di \\
  Carnegie Mellon University\\
  \texttt{sarahdi@andrew.cmu.edu} \\\And
  Jinsol Park \\
  Carnegie Mellon University\\
  \texttt{jinsolp@andrew.cmu.edu} \\}

\begin{document}
\maketitle
\begin{abstract}
We implemented sparse attention on CPU and GPU platforms using C++ and CUDA respectively, and compared the performance of the two implementations.

INCLUDE:
additional project deliverables 
"Our implementation achieved X speedup"
"Given the speed of our implementation, we demonstrate that a X approach to sparse attention is effective"
What machines they ran on

\end{abstract}

\section{Background}
\cite{beltagy2020longformer}

Describe the algorithm, application, or system you parallelized in computer science terms. Figures would be really useful here.
• What are the key data structures?
• What are the key operations on these data structures?
• What are the algorithm’s inputs and outputs?
• What is the part that computationally expensive and could benefit from parallelization?
• Break down the workload. Where are the dependencies in the program? How much parallelism is there? Is it data-parallel? Where is the locality? Is it amenable to SIMD execution?

\begin{table*}[h!]
\centering
 \begin{tabular}{||c | c c c c||} 
 \hline
 SeqLen & Attention & Add\&Norm & FeedForward & Add\&Norm \\ [0.5ex] 
 \hline\hline
 128 & 0.079665 & 0.019992 & 0.084192 & 0.073205 \\
 256 & 0.300243 & 0.040303 & 0.153612 & 0.147656 \\
 512 & 0.545014 & 0.078534 & 0.322093 & 0.282523 \\
 1024 & 3.854414 & 0.193473 & 0.686376 & 0.707752 \\
 2048 & 13.631169 & 0.332429 & 1.277496 & 1.172507 \\ [1ex] 
 \hline
 \end{tabular}
  \caption{Caption}
  \label{fig:enter-label}
\end{table*}


\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}}

\section{Approach}

Tell us how your implementation works. Your description should be sufficiently detailed to provide the course staff a basic understanding of your approach. Again, it might be very useful to include a figure here illustrating components of the system and/or their mapping to parallel hardware. 

• Describe the technologies used. What language/APIs? What machines did you target?

• Describe how you mapped the problem to your target parallel machine(s). IMPORTANT: How do the data structures and operations you described in part 2 map to machine concepts like cores and threads. (or warps, thread blocks, gangs, etc.)

• Did you change the original serial algorithm to enable better mapping to a parallel machine?

• If your project involved many iterations of optimization, please describe this process as well. What did you try that did not work? How did you arrive at your solution? The notes you’ve been writing throughout your project should be helpful here. Convince us you worked hard to arrive at a good solution.

• If you started with an existing piece of code, please mention it (and where it came
from) here.

\section{Results}

How successful were you at achieving your goals? We expect results sections to differ from project to project, but we expect your evaluation to be very thorough (your project evaluation is a great way to demonstrate you understood topics from this course).

Here are a few ideas:

• If your project was optimizing an algorithm, please define how you measured performance. Is it wall-clock time? Speedup? An application specific rate? (e.g., moves per second, images/sec)

• Please also describe your experimental setup. What were the size of the inputs? How were requests generated?

• Provide graphs of speedup or execute time. Please precisely define the configurations being compared. Is your baseline single-threaded CPU code? It is an optimized parallel implementation for a single CPU?

• Recall the importance of problem size. Is it important to report results for different problem sizes for your project? Do different workloads exhibit different execution behavior?

• IMPORTANT: What limited your speedup? Is it a lack of parallelism? (dependencies) Communication or synchronization overhead? Data transfer (memory-bound or bus transfer bound). Poor SIMD utilization due to divergence? As you try and answer these questions, we strongly prefer that you provide data and measurements to support your conclusions. If you are merely speculating, please state this explicitly. Performing a solid analysis of your implementation is a good way to pick up credit even if your optimization efforts did not yield the performance you were hoping for.

\subsection{Deeper Analysis}
• Deeper analysis: Can you break execution time of your algorithm into a number of distinct components. What percentage of time is spent in each region? Where is there room to improve?

• Was your choice of machine target sound? (If you chose a GPU, would a CPU have been a better choice? Or vice versa.)

\section{List of work by each student, Distribution of Total Credit}
Please list the work performed by each partner. Given that you worked in a
group, how should the total credit for the project be distributed amongst the participants? If you do not feel comfortable placing this information on a public web page, it is okay to include it only on the version that you submit via Gradescope.


% Entries for the entire Anthology, followed by custom entries
\bibliographystyle{plain}
\bibliography{FinalReport/custom}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is a section in the appendix.

\end{document}