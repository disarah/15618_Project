@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{lin2022survey,
  title={A survey of transformers},
  author={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  journal={AI Open},
  year={2022},
  publisher={Elsevier}
}

@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

@misc{cuBLAS,
  title = {Nvidia cuBLAS Library},
  author={Nvidia},
  howpublished = {\url{http://www.nvidia.com/object/cuda develop.html}},
    year={2010}
}

@inproceedings{bell2009implementing,
  title={Implementing sparse matrix-vector multiplication on throughput-oriented processors},
  author={Bell, Nathan and Garland, Michael},
  booktitle={Proceedings of the conference on high performance computing networking, storage and analysis},
  pages={1--11},
  year={2009}
}

@inproceedings{bulucc2009parallel,
  title={Parallel sparse matrix-vector and matrix-transpose-vector multiplication using compressed sparse blocks},
  author={Bulu{\c{c}}, Aydin and Fineman, Jeremy T and Frigo, Matteo and Gilbert, John R and Leiserson, Charles E},
  booktitle={Proceedings of the twenty-first annual symposium on Parallelism in algorithms and architectures},
  pages={233--244},
  year={2009}
}

@article{giannoula2022sparsep,
  title={Sparsep: Towards efficient sparse matrix vector multiplication on real processing-in-memory architectures},
  author={Giannoula, Christina and Fernandez, Ivan and Luna, Juan G{\'o}mez and Koziris, Nectarios and Goumas, Georgios and Mutlu, Onur},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume={6},
  number={1},
  pages={1--49},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@misc{blockedEll,
  title = {Accelerating Matrix Multiplication with Block Sparse Format and Nvidia Tensor Cores},
  author={Yamaguchi,Takuma and Busato, Federico},
    howpublished = {\url{https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/}},
    year={2021}
}